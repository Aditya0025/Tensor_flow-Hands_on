{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "model = tf.keras.Sequential([])\n",
    "\n",
    "#optmizer\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of a Tensor defines its number of dimensions \n",
    "and the size of each dimension. The rank of a Tensor \n",
    "provides the number of dimensions (n-dimensions) --\n",
    "you can also think of this as the \n",
    "Tensor's order or degree"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0-d Tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport = tf.constant(\"Tennis\",tf.string)\n",
    "number = tf.constant(1.41421,tf.float64)\n",
    "print(sport,\"`sport` is int {}-d Tensor\".format(tf.rank(sport).numpy()))\n",
    "print(number,\"`number` is a {}-d Tensor\".format(tf.rank(number).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vectors and lists can be used to create 1-d Tensors:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1-d Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\n",
    "numbers = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next we consider creating 2-d (i.e., matrices) and \n",
    "higher-rank Tensors. For examples, in future labs \n",
    "involving image processing and computer vision, we will use 4-d Tensors. Here the dimensions correspond to the number of example images in our batch, image height, \n",
    "image width, and the number of color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define higher-order Tensor\n",
    "\n",
    "'''Todo: define a 2-d Tensor'''\n",
    "matrix = #todo\n",
    "\n",
    "assert isinstance(images,tf.Tensor), \"matrix must be a tf.Tensor object\"\n",
    "assert tf.rank(images).numpy()==2,\"matrix must be of rank 2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Todo: Define a 4-d Tensor'''\n",
    "# Use tf.zeros to initialize a 4-d Tensor of zeros with size 10 x 256 x 256 x 3. \n",
    "#   You can think of this as 10 images where each image is RGB 256 x 256.\n",
    "\n",
    "images= # TODO\n",
    "\n",
    "assert isinstance(images,tf.Tensor),\"matrx must br a tf Tenssor object\"\n",
    "assert tf.rank(images).numpy == 4,\"matrix must be of rank 4\"\n",
    "assert tf.shape(images).numpy.tolist()==[10,256,256,3],\"matrix is incorect shape\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As you have seen, the shape of a Tensor provides the \n",
    "number of elements in each Tensor dimension. \n",
    "The shape is quite useful, and we'll use it often. \n",
    "You can also use slicing to access \n",
    "subtensors within a higher-rank Tenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_vector= matrix[1]\n",
    "column_vector = matrix[:,2]\n",
    "scalar = matrix[1,2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Computations on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the nodes in the graph, and intialize values\n",
    "a= tf.constant(15)\n",
    "b= tf.constant(61)\n",
    "\n",
    "c1 = tf.add(a,b)\n",
    "c2= a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Neural networks in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a network layer\n",
    "\n",
    "class OurDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,n_output_nodes):\n",
    "        super(OurDenseLayer,self).__init()\n",
    "        self.n_output_nodes=n_output_nodes\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        d= int(input_shape[-1])\n",
    "#         define and intialize paramters: a weight matrix w and bias b\n",
    "#         NOte that parmeter  intlaization is random\n",
    "        self.w= self.add_weight(\"weight\",shape=[d,self.n_output_nodes]) #NOte the dimensionality\n",
    "        self.b- self.add_weight(\"bias\",hape[1,self.n_output_nodes]) #Note the dim\n",
    "    \n",
    "    def call(self,x):\n",
    "        ''' TODO: define the operation fr z'''\n",
    "        z= # todo\n",
    "        \n",
    "        '''Todo: define the operation for out'''\n",
    "        y =# todo\n",
    "        \n",
    "# since layer parmater are intlialized randomly,we will set radom seed for\n",
    "# reproducibilit\n",
    "tf.random.set_seed(1)\n",
    "layer= OurDenseLayer(3)\n",
    "layer.build((1,2))\n",
    "x_input= tf.contant([[1,2.]], shape=(1,2))\n",
    "y = layer.call(x_input)\n",
    "\n",
    "\n",
    "# test the output\n",
    "print(y.numpy)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use the Sequential model from Keras and a single Dense layer to define our network. With the Sequential API, you can readily create neural networks by stacking together \n",
    "layers like building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the number of outputs\n",
    "n_output_nodes = 3\n",
    "\n",
    "# First define the model \n",
    "model = Sequential()\n",
    "\n",
    "dense_layer = # TODO\n",
    "\n",
    "# Add the dense layer to the model\n",
    "model.add(dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Test model with example input\n",
    "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
    "\n",
    "'''TODO: feed input into the model and predict the output!'''\n",
    "model_output = # TODO\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to defining models using the Sequential API, \n",
    "we can also define neural networks by directly \n",
    "subclassing the Model class, which groups layers \n",
    "together to enable model training and inference. \n",
    "The Model class captures what we refer to as a \"model\" \n",
    "or as a \"network\". Using Subclassing, we can create a \n",
    "class for our model, and then define the forward pass \n",
    "through the network using the call function. Subclassing \n",
    "affords the flexibility to define custom layers, custom \n",
    "training loops, custom activation functions, and \n",
    "custom models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define the same neural network as above now using Subclassing rather than the Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining the model/nework usig sublass\n",
    "\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers impotr Dense\n",
    "\n",
    "\n",
    "class SubclassModel(tf.keras.Model):\n",
    "    #in  __init we define th model layer\n",
    "    def  __init__(self,n_output_nodes):\n",
    "        super(SubclassModel, self).__init__()\n",
    "        '''Todo : our model consit of single dense layer \n",
    "        .Define this layer'''\n",
    "        \n",
    "        self.Dense_layer= '''Dnse_layer'''\n",
    "        \n",
    "        #call the function we definr the models forward pass\n",
    "        def call(self,inputs):\n",
    "            return self.dense_layer(inputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Just like the model we built using the Sequential API,\n",
    "let's test out our SubclassModel using an example input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output_nodes = 3\n",
    "model = SubclassModel(n_output_nodes)\n",
    "\n",
    "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
    "\n",
    "print(model.call(x_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
